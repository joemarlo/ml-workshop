---
title: "Tidymodels"
author: "Jared"
date: "`r Sys.Date()`"
output:
    html_document:
        toc: true
        toc_float:
            collapsed: false
            smooth_scroll: true
        number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)

# for those with dark rstudio themes
options(tidymodels.dark = TRUE)
```

# Packages {#packages}

```{r load-packages}
library(dplyr)
library(ggplot2)
library(rsample)
library(recipes)
library(parsnip)
library(workflows)
library(yardstick)
library(tune)
library(dials)
library(tictoc)
library(doFuture)
library(parallel)
library(vip)
library(vetiver)
```

# Data {#data}

```{r load-data}
data(credit_data, package='modeldata')
credit <- as_tibble(credit_data)

credit
```

# Create Fake "New" Data {#FakeData}

```{r fake-data}
fake <- credit |> 
    slice_sample(n=10) |> 
    select(-Status)
fake
```

# Split into Train and Test {#TrainTestsplit}

`{rsample}`

```{r split-data}
set.seed(1234)
sample(10, 4)

set.seed(1234)
credit_split <- initial_split(credit, prop=0.85, strata='Status')
credit_split

train <- training(credit_split)
test <- testing(credit_split)

train
test
```

# Preprocess Data AKA Feature Engineering {#recipes}

Load `{recipes}`

```{r check-balance}
ggplot(train, aes(x=Status)) + geom_bar()
```


```{r recipe}
recipe(Status ~ Income + Home, data=train) |> 
    step_mutate_at(all_nominal_predictors(), fn = ~ coalesce(.x, "Missing")) |> 
    step_string2factor(Home) |> 
    step_dummy(Home, one_hot=TRUE) |>
    prep() |> 
    bake(new_data=NULL)

rec1 <- recipe(Status ~ ., data=train) |>
    step_rm(Time) |> 
    # remove columns that have little variance
    step_nzv(all_predictors()) |> 
    # not necessary with xgboost
    themis::step_upsample(Status) |>
    # not necessary with xgboost
    step_impute_knn(all_numeric_predictors()) |> 
    # not necessary with xgboost
    # step_factor2string(all_nominal_predictors()) |> 
    # step_mutate(Home=tidyr::replace_na(Home, 'missing')) |> 
    # step_mutate(Job=tidyr::replace_na(Job, 'missing')) |> 
    # step_mutate(Marital=tidyr::replace_na(Marital, 'missing')) |> 
    # step_mutate_at(
    #     all_nominal_predictors(), fn = ~ coalesce(.x, "Missing")
    # ) |> 
    # step_string2factor(all_nominal_predictors()) |> 
    # same as above
    # step_string2factor(Home, Job, Marital) |> 
    # same as coalesce and step_mutate
    step_unknown(all_nominal_predictors()) |> 
    # same as above
    step_discretize(Age) |> 
    # not necessary for xgboost
    step_normalize(all_numeric_predictors()) |> 
    # this is needed for both glmnet and xgboost
    step_other(all_nominal_predictors(), threshold=0.1, other='misc') |>
    step_novel(all_nominal_predictors()) |> 
    step_dummy(all_nominal_predictors(), one_hot=TRUE)
```

# Define our Model {#DefineModel}

`{parsnip}`

```{r define-model}
linear_reg()
boost_tree()
rand_forest()

linear_reg() |> set_engine('lm')
linear_reg() |> set_engine('glmnet')
linear_reg() |> set_engine('stan')
linear_reg() |> set_engine('brulee')

boost_tree() |> set_engine('xgboost')

show_model_info('boost_tree')

spec1 <- boost_tree(
    mode='classification', 
    trees=100, tree_depth=4, sample_size=0.7
) |> 
    set_engine('xgboost')
spec1
```

Explore model types at https://www.tidymodels.org/find/parsnip/

# Combine our Recipe and Model {#workflow}

`{workflows}`

```{r create-workflow}
flow1 <- workflow() |> 
    add_recipe(rec1) |> 
    add_model(spec1)
flow1
```

# Train Model {#TrainModel}

```{r train-model-1}
mod1 <- fit(flow1, data=train)
mod1
```

```{r score-1}
fake
predict(mod1, new_data=fake)
predict(mod1, new_data=fake, type='prob')
```

# How Good is My Model? {#ModelQuality}

`{rsample}`

```{r set-up-cross-validation}
cv_set <- vfold_cv(data=train, v=5, repeats=1, strata='Status')
cv_set
cv_set$splits
cv_set$splits[[1]]
cv_set$splits[[1]] |> training()
cv_set$splits[[1]] |> testing()
```

`{yardstick}`

```{r assess-mod-1}
metrics1 <- metric_set(mn_log_loss, roc_auc)
metrics1
```

`{tune}`

```{r run-cross-validation}
cv1 <- fit_resamples(
    flow1, 
    resamples=cv_set, 
    metrics=metrics1, 
    control=control_resamples(verbose=TRUE)
)

cv1
cv1$.metrics

cv1 |> collect_metrics()
```

# Try Different Parameters {#DiffParams}

```{r diff-params}
spec2 <- boost_tree(
    mode='classification', 
    trees=200, tree_depth=2, sample_size=0.7
) |> 
    set_engine('xgboost')
spec2

# flow2 <- workflow() |> add_recipe(rec1) |> add_model(spec2)
flow2 <- flow1 |> update_model(spec2)
flow2
```

```{r cv-2}
cv2 <- fit_resamples(
    flow2,
    resamples=cv_set,
    metrics=metrics1,
    control=control_resamples(verbose=TRUE)
)

cv1 |> collect_metrics()
cv2 |> collect_metrics()
```

# How do we Choose the Best Model Parameters? {#ChooseParams}

`{dials}`

```{r tunable-recipe}
# skipped rec2
rec3 <- recipe(Status ~ ., data=train) |>
    step_rm(Time) |> 
    # remove columns that have little variance
    step_nzv(all_predictors()) |> 
    # not necessary with xgboost
    themis::step_upsample(Status) |>
    # not necessary with xgboost
    step_impute_knn(all_numeric_predictors()) |> 
    step_unknown(all_nominal_predictors()) |> 
    # same as above
    step_discretize(Age) |> 
    # this is needed for both glmnet and xgboost
    step_other(all_nominal_predictors(), threshold=tune(), other='misc') |>
    step_novel(all_nominal_predictors()) |> 
    step_dummy(all_nominal_predictors(), one_hot=TRUE)
rec3
```

```{r tunable-model-spec}
spec3 <- boost_tree(
    mode='classification',
    trees=tune(),
    tree_depth=tune(),
    sample_size=0.7
) |> 
    set_engine('xgboost')
spec3
```

```{r tunable-workflow}
flow3 <- flow2 |> update_recipe(rec3) |> update_model(spec3)
flow3
```

```{r set-parameter-ranges}
tune_args(flow3)
flow3 |> 
    parameters()

tunable(flow3)

flow3 |> extract_parameter_set_dials() |> select(object) |> magrittr::extract2(1)

params3 <- flow3 |> 
    extract_parameter_set_dials() |> 
    update(
        trees=trees(range=c(50, 300)),
        tree_depth=tree_depth(range=c(2, 8))
    )
params3
params3$object
```

```{r param-combinations}
grid3 <- grid_random(params3, size=50)
grid3
```

`{tictoc}`

```{r grid-search}
tic(msg='Search 3')
search3 <- tune_grid(
    flow3,
    resamples=cv_set,
    grid=grid3,
    metrics=metrics1,
    control=control_grid(verbose=TRUE)
)
toc(log=TRUE)

tic.log()
```

```{r grid-check}
search3
search3$.metrics[[1]]
search3 |> collect_metrics()
search3 |> autoplot()
search3 |> autoplot(metric='roc_auc')

search3 |> show_best(metric='roc_auc', n=3)
```


```{r check-cores}
parallelly::availableCores()
parallelly::availableCores(logical=FALSE)
parallelly::availableCores(logical=TRUE)
parallelly::availableWorkers(logical=FALSE)
```

`{doFuture}` and `{parallel}`

```{r register-backend}
registerDoFuture()
cl <- makeCluster(8)
cl
plan(cluster, workers=cl)
```

```{r grid-search-parallel}
tic(msg='Search 3 Parallel')
search3_parallel <- tune_grid(
    flow3,
    resamples=cv_set,
    grid=grid3,
    metrics=metrics1,
    control=control_grid(verbose=TRUE, allow_par=TRUE, parallel_over='everything')
)
toc(log=TRUE)

tic.log()
```


```{r pipe-comparison}
# data %>% lm(y~x, data=.)
# data |> lm(y~x, data=_)

substitute(x |> sum())
substitute(x %>% sum())
```

# Choose the Best Model {#BestModel}

```{r choose-best}
search3 |> show_best(metric='roc_auc')
search3 |> select_best(metric='roc_auc')

chosen_params <- search3 |> select_by_one_std_err(metric='roc_auc', trees)
chosen_params
```


```{r set-params}
flow4 <- finalize_workflow(flow3, parameters=chosen_params)

flow3
flow4
```

```{r fit-chosen-model}
mod4 <- fit(flow4, data=train)
mod4

preds4 <- predict(mod4, new_data=test, type='prob')
preds4.1 <- predict(mod4, new_data=test, type='class')
preds4
preds4.1

bind_cols(preds4.1, test |> select(Status)) |> accuracy(Status, .pred_class)

check4 <- last_fit(flow4, split=credit_split, metrics=metrics1)
check4
check4$.metrics
```

# Understand the Model (As Best we Can) {#UnderstandModel}

`{vip}`

```{r vip}
mod4 |> extract_fit_engine() |> vip()
```

# Fit Model on Entire Data {#FitAllData}

```{r fit-all-data}
mod_full <- fit(flow4, data=credit)
mod_full

predict(mod_full, new_data=fake[1, ], type='prob')
predict(mod_full, new_data=fake[2, ], type='prob')
predict(mod_full, new_data=fake[3:4, ], type='prob')
```

# Convert Model to API {#ConvertModelToAPI}

`{vetiver}`

```{r make-api}
v <- vetiver_model(mod4, model_name='credit_scorer', description='XGBoost model', TRUE)
v
v$model
v$ptype


the_board <- pins::board_temp()
the_board

vetiver_pin_write(the_board, v)

vetiver_write_plumber(the_board, name='credit_scorer', file='mod4.r')
```

